#!/usr/bin/env python3
"""
ğŸŒ¸ SakuraLLM Integration Example

This script demonstrates how to use SakuraLLM for high-quality 
Japaneseâ†’Chinese translation in the Sakura Subtitle Generator.
"""

import sys
import os

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.translation.pytorch_translator import PyTorchTranslator
from src.utils.logger import setup_logger

def main():
    """Demonstrate SakuraLLM translation."""
    
    # Setup logging
    logger = setup_logger("sakura_example")
    
    # Japanese text samples (typical anime/light novel content)
    test_texts = [
        "ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ",
        "ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚",
        "ã‚¢ãƒ‹ãƒ¡ã‚’è¦‹ã‚‹ã®ãŒå¥½ãã§ã™ã€‚",
        "ã“ã®å°èª¬ã¯ã¨ã¦ã‚‚é¢ç™½ã„ã§ã™ã€‚",
        "å½¼ã¯å­¦æ ¡ã«è¡Œãã¾ã—ãŸã€‚"
    ]
    
    print("ğŸŒ¸ SakuraLLM Translation Example")
    print("=" * 50)
    
    try:
        # Initialize SakuraLLM translator
        print("Initializing SakuraLLM translator...")
        translator = PyTorchTranslator(
            model_name="SakuraLLM/Sakura-1.5B-Qwen2.5-v1.0-GGUF", 
            source_lang="ja",
            target_lang="zh",
            device="auto",  # Auto-detect GPU
            torch_dtype="float16",
            force_gpu=True,
            batch_size=4
        )
        
        # Load model
        print("Loading model (this may take a few minutes for first download)...")
        if not translator.load_model():
            print("âŒ Failed to load SakuraLLM model")
            return
            
        print("âœ… SakuraLLM loaded successfully!")
        print(f"ğŸ”§ Using device: {translator.optimal_device}")
        print()
        
        # Translate test texts
        print("ğŸŒ¸ Translation Results:")
        print("-" * 50)
        
        for i, text in enumerate(test_texts, 1):
            print(f"{i}. Japanese: {text}")
            
            # Translate
            result = translator.translate_text(text)
            
            print(f"   Chinese:  {result.translated_text}")
            print(f"   Quality:  {result.confidence:.2f}")
            print()
        
        # Cleanup
        translator.unload_model()
        print("ğŸ¯ Translation completed successfully!")
        
    except Exception as e:
        logger.error(f"Error during translation: {e}")
        print(f"âŒ Error: {e}")
        
        # Fallback suggestion
        print("\nğŸ’¡ Troubleshooting:")
        print("- Ensure you have sufficient VRAM/RAM (4GB+ required)")
        print("- Check internet connection for model download")
        print("- Try with force_gpu=False for CPU fallback")

def compare_backends():
    """Compare SakuraLLM vs Helsinki-NLP translation quality."""
    
    print("\nğŸ”„ Backend Comparison")
    print("=" * 50)
    
    test_text = "å½¼å¥³ã¯ç¾ã—ã„æ¡œã®èŠ±ã‚’è¦‹ã¦ã„ã¾ã™ã€‚"
    
    try:
        # SakuraLLM translation
        print("ğŸŒ¸ SakuraLLM Translation:")
        sakura = PyTorchTranslator(
            model_name="SakuraLLM/Sakura-1.5B-Qwen2.5-v1.0-GGUF",
            source_lang="ja", target_lang="zh",
            device="auto", force_gpu=True
        )
        
        if sakura.load_model():
            result_sakura = sakura.translate_text(test_text)
            print(f"Input:  {test_text}")
            print(f"Output: {result_sakura.translated_text}")
            sakura.unload_model()
        
        # Helsinki-NLP comparison (would need to implement)
        print("\nğŸ“Š Helsinki-NLP Translation:")
        print("Input:  å½¼å¥³ã¯ç¾ã—ã„æ¡œã®èŠ±ã‚’è¦‹ã¦ã„ã¾ã™ã€‚")
        print("Output: [Implement HuggingFaceTranslator comparison]")
        
    except Exception as e:
        print(f"âŒ Comparison failed: {e}")

if __name__ == "__main__":
    main()
    
    # Optional: Run backend comparison
    # compare_backends()